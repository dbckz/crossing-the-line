{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "create_hatebase_regression_file.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1ypjRKdqqUyS",
        "ngzVB2h4x6J9",
        "oLGzXt3cG1_x"
      ],
      "authorship_tag": "ABX9TyNOo7Dfqh5eGaEHGmrTFjjZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbckz/dissertation/blob/master/notebooks/create_hatebase_regression_file.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br4Ph0_Tpojf"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLv78bpwk10K"
      },
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import plotly.graph_objects as go"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx3CYfwcoyEw",
        "outputId": "ecec147d-1f47-4693-8e4a-d28394ddd8d0"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnNZcE26nZ_v"
      },
      "source": [
        "# Set up paths\n",
        "root_path = \"/content/drive/MyDrive/University/Dissertation/data_collection\"\n",
        "graph_path = root_path + \"/graphs\"\n",
        "\n",
        "day_paths = day_paths = [\n",
        "        \"/01\",\n",
        "        \"/02\",\n",
        "        \"/03\",\n",
        "        \"/04\",\n",
        "        \"/05\",\n",
        "        \"/06\",\n",
        "        \"/07\",\n",
        "        \"/08\",\n",
        "        \"/09\",\n",
        "        \"/10\",\n",
        "        \"/11\",\n",
        "        \"/12\",\n",
        "        \"/13\",\n",
        "        \"/14\",\n",
        "        \"/15\",\n",
        "        \"/16\",\n",
        "        \"/17\",\n",
        "        \"/18\",\n",
        "        \"/19\",\n",
        "        \"/20\",\n",
        "        \"/21\",\n",
        "        \"/22\",\n",
        "        \"/23\",\n",
        "        \"/24\",\n",
        "        \"/25\",\n",
        "        \"/26\",\n",
        "        \"/27\",\n",
        "        \"/28\",\n",
        "        \"/29\",\n",
        "        \"/30\",\n",
        "        \"/31\",\n",
        "        \"/32\",\n",
        "        \"/33\",\n",
        "        \"/34\",\n",
        "        \"/35\",\n",
        "        \"/36\"\n",
        "    ]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaaxCbrhnxib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbfa685d-4312-4afd-8920-f4cd496adb0e"
      },
      "source": [
        "# Create directory to store visualisations\n",
        "try:\n",
        "    os.mkdir(graph_path)\n",
        "except OSError as error:\n",
        "    print(error)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 17] File exists: '/content/drive/MyDrive/University/Dissertation/data_collection/graphs'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC_0fxX3oAKF",
        "outputId": "ef6644dc-243a-40a1-989f-a9b6aa446c4b"
      },
      "source": [
        "# Load data\n",
        "threshold = 90\n",
        "\n",
        "in_tweets = pd.DataFrame()\n",
        "hb_guard = pd.DataFrame()\n",
        "for path in day_paths:\n",
        "    directory = root_path + path\n",
        "    tweets_csv = directory + \"/tweets.csv\"\n",
        "    matched_terms_csv = directory + \"/hatebase_processed_tweets.csv\"\n",
        "\n",
        "    print(f\"Loading CSVs for directory {path}...\")\n",
        "    in_tweets = pd.concat([in_tweets, \n",
        "                           pd.read_csv(tweets_csv,\n",
        "                                       usecols = [\n",
        "                                                  'created_at',\n",
        "                                                  'tweet_id',\n",
        "                                                  'tweet_text',\n",
        "                                                  'accounts_mentioned'\n",
        "                                       ],\n",
        "                                       dtype = {\n",
        "                                          # 'created_at':\n",
        "                                          'tweet_id': np.int64,\n",
        "                                          'tweet_text': str,\n",
        "                                          'accounts_mentioned': object\n",
        "                                       },\n",
        "                                       parse_dates=['created_at'])])\n",
        "\n",
        "    hb_guard = pd.concat([hb_guard, pd.read_csv(matched_terms_csv,\n",
        "                                                usecols = [\n",
        "                                                           'tweet_id',\n",
        "                                                           f'matching_hatebase_terms_over_{threshold}'\n",
        "                                                ],\n",
        "                                                dtype = {\n",
        "                                                    'tweet_id': np.int64,\n",
        "                                                    f'matching_hatebase_terms_over_{threshold}': str\n",
        "                                                })])\n",
        "\n",
        "# Dedup\n",
        "original_tweets_length = len(in_tweets)\n",
        "original_hatebase_length = len(hb_guard)\n",
        "in_tweets.drop_duplicates(subset=['tweet_id'], inplace=True)\n",
        "hb_guard.drop_duplicates(subset=['tweet_id'], inplace=True)\n",
        "print(f\"Size of tweets dataframe: {len(in_tweets)}, having dropped {original_tweets_length - len(in_tweets)} duplicate rows\")\n",
        "print(f\"Size of hatebase dataframe: {len(hb_guard)}, having dropped {original_hatebase_length - len(hb_guard)} duplicate rows\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading CSVs for directory /01...\n",
            "Loading CSVs for directory /02...\n",
            "Loading CSVs for directory /03...\n",
            "Loading CSVs for directory /04...\n",
            "Loading CSVs for directory /05...\n",
            "Loading CSVs for directory /06...\n",
            "Loading CSVs for directory /07...\n",
            "Loading CSVs for directory /08...\n",
            "Loading CSVs for directory /09...\n",
            "Loading CSVs for directory /10...\n",
            "Loading CSVs for directory /11...\n",
            "Loading CSVs for directory /12...\n",
            "Loading CSVs for directory /13...\n",
            "Loading CSVs for directory /14...\n",
            "Loading CSVs for directory /15...\n",
            "Loading CSVs for directory /16...\n",
            "Loading CSVs for directory /17...\n",
            "Loading CSVs for directory /18...\n",
            "Loading CSVs for directory /19...\n",
            "Loading CSVs for directory /20...\n",
            "Loading CSVs for directory /21...\n",
            "Loading CSVs for directory /22...\n",
            "Loading CSVs for directory /23...\n",
            "Loading CSVs for directory /24...\n",
            "Loading CSVs for directory /25...\n",
            "Loading CSVs for directory /26...\n",
            "Loading CSVs for directory /27...\n",
            "Loading CSVs for directory /28...\n",
            "Loading CSVs for directory /29...\n",
            "Loading CSVs for directory /30...\n",
            "Loading CSVs for directory /31...\n",
            "Loading CSVs for directory /32...\n",
            "Loading CSVs for directory /33...\n",
            "Loading CSVs for directory /34...\n",
            "Loading CSVs for directory /35...\n",
            "Loading CSVs for directory /36...\n",
            "Size of tweets dataframe: 1478009, having dropped 133 duplicate rows\n",
            "Size of hatebase dataframe: 1478009, having dropped 133 duplicate rows\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIQbp7k92PQ_"
      },
      "source": [
        "# Up the pandas display limits so printed dataframes aren't so truncated\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_info_rows', 100)\n",
        "pd.set_option('display.max_info_columns', 100)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKwJiJz_pvQ9"
      },
      "source": [
        "# Data manipulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHz_MRv9p1P9"
      },
      "source": [
        "# Join tables + drop old ones!\n",
        "joined_df = pd.merge(in_tweets, hb_guard, how='outer', on='tweet_id')\n",
        "del hb_guard\n",
        "del in_tweets"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc1jTlRO4js5",
        "outputId": "76c5cf8e-aa68-406e-ac99-6b1c4f661e16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "joined_df = joined_df[(joined_df['created_at'] > '2021-06-19 08:10:18+00:00') & (joined_df['created_at'] < '2021-07-17 00:00:00+00:00')]\n",
        "len(joined_df)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1274885"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpfkGRwSqBzL"
      },
      "source": [
        "# Create a column indicating whether tweet contains slurs (hacky > 2 as empty list is stored as string \"[]\")\n",
        "joined_df['contains_slurs'] = joined_df[f'matching_hatebase_terms_over_{threshold}'].str.len() > 2\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2wlAG3DvVEC",
        "outputId": "433cdeb3-4657-45be-f076-2f6a6b6218e5"
      },
      "source": [
        "# Extract players\n",
        "england = [\"JPickford1\", \"kylewalker2\", \"LukeShaw23\", \"_DeclanRice\", \"HarryMaguire93\", \"JackGrealish\",\n",
        "                    \"JHenderson\", \"HKane\", \"sterling7\", \"MarcusRashford\", \"trippier2\", \"deanhenderson\",\n",
        "                    \"Kalvinphillips\", \"OfficialTM_3\", \"Sanchooo10\", \"CalvertLewin14\", \"masonmount_10\", \"PhilFoden\",\n",
        "                    \"BenChilwell\", \"ben6white\", \"samjohnstone50\", \"reecejames_24\", \"BukayoSaka87\", \"BellinghamJude\"]\n",
        "\n",
        "netherlands = [\"joel_veltman\", \"mdeligt_04\", \"NathanAke\", \"Stefandevrij\", \"GWijnaldum\", \"LuukdeJong9\", \"Memphis\", \"QPromes\", \"pvanaanholt\", \"TimKrul\", \"DavyKlaassen\", \"Dirono\", \"RGravenberch\", \"BlindDaley\", \"DeJongFrenkie21\", \"DenzelJMD2\"]\n",
        "\n",
        "germany = [\"Manuel_Neuer\", \"ToniRuediger\", \"MatzeGinter\", \"matshummels\", \"kaihavertz29\", \"ToniKroos\", \"KeVolland\", \"SergeGnabry\", \"Bernd_Leno\", \"JamalMusiala\", \"lukaskl96\", \"leongoretzka_\", \"leroy_sane\", \"IlkayGuendogan\", \"emrecan_\", \"RobinKoch25\", \"esmuellert_\"]\n",
        "\n",
        "scotland = [\"MarshallDavid23\", \"sodonnell15\", \"andrewrobertso5\", \"mctominay10\", \"granthanley5\", \"kierantierney1\", \"jmcginn7\", \"Callummcgregor8\", \"Lyndon_Dykes\", \"CheAdams_\", \"CraigGordon01\", \"declang31\", \"LiamCooper__\", \"10DavidTurnbull\", \"kevinnisbet16\", \"np4tterson\", \"billygilmourrr\", \"Jack_Hendry2\", \"Scottmckenna3\"]\n",
        "\n",
        "france = [\"BenPavard28\", \"kimpembe_3\", \"raphaelvarane\", \"clement_lenglet\", \"paulpogba\", \"AntoGriezmann\", \"_OlivierGiroud_\", \"KMbappe\", \"CorentinTolisso\", \"nglkante\", \"KurtZouma\", \"SteveMandanda\", \"MoussaSissoko\", \"LucasDigne\", \"Benzema\", \"LucasHernandez\", \"WissBenYedder\", \"mmseize\", \"leodubois15\", \"jkeey4\", \"MarcusThuram\"]\n",
        "\n",
        "belgium = [\"thibautcourtois\", \"AlderweireldTob\", \"thomasvermaelen\", \"JanVertonghen\", \"axelwitsel28\", \"DeBruyneKev\", \"RomeluLukaku9\", \"hazardeden10\", \"CarrascoY21\", \"SMignolet\", \"dries_mertens14\", \"ThomMills\", \"HazardThorgan8\", \"VanakenHans\", \"Jasondenayer\", \"chrisbenteke\", \"NChadli\", \"mbatshuayi\", \"LTrossard\", \"JeremyDoku\", \"dennispraet\"]\n",
        "\n",
        "list_of_players = england + netherlands + germany + scotland + france + belgium\n",
        "\n",
        "for player in list_of_players:\n",
        "    print(f\"Extracting {player}...\")\n",
        "    joined_df[player] = joined_df['accounts_mentioned'].str.contains(f\"'username': '{player}'\").astype(np.bool)\n",
        "\n",
        "# player_tweet_map = pd.DataFrame(columns=[\"username\", \"tweets_received\"])\n",
        "\n",
        "# i = 0\n",
        "# for player in list_of_players:\n",
        "#     tweets = joined_df[player].sum()\n",
        "#     player_tweet_map.loc[i] = player, tweets\n",
        "#     i += 1\n",
        "\n",
        "# player_tweet_map.sort_values('tweets_received', axis=0, ascending=False, inplace=True)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting JPickford1...\n",
            "Extracting kylewalker2...\n",
            "Extracting LukeShaw23...\n",
            "Extracting _DeclanRice...\n",
            "Extracting HarryMaguire93...\n",
            "Extracting JackGrealish...\n",
            "Extracting JHenderson...\n",
            "Extracting HKane...\n",
            "Extracting sterling7...\n",
            "Extracting MarcusRashford...\n",
            "Extracting trippier2...\n",
            "Extracting deanhenderson...\n",
            "Extracting Kalvinphillips...\n",
            "Extracting OfficialTM_3...\n",
            "Extracting Sanchooo10...\n",
            "Extracting CalvertLewin14...\n",
            "Extracting masonmount_10...\n",
            "Extracting PhilFoden...\n",
            "Extracting BenChilwell...\n",
            "Extracting ben6white...\n",
            "Extracting samjohnstone50...\n",
            "Extracting reecejames_24...\n",
            "Extracting BukayoSaka87...\n",
            "Extracting BellinghamJude...\n",
            "Extracting joel_veltman...\n",
            "Extracting mdeligt_04...\n",
            "Extracting NathanAke...\n",
            "Extracting Stefandevrij...\n",
            "Extracting GWijnaldum...\n",
            "Extracting LuukdeJong9...\n",
            "Extracting Memphis...\n",
            "Extracting QPromes...\n",
            "Extracting pvanaanholt...\n",
            "Extracting TimKrul...\n",
            "Extracting DavyKlaassen...\n",
            "Extracting Dirono...\n",
            "Extracting RGravenberch...\n",
            "Extracting BlindDaley...\n",
            "Extracting DeJongFrenkie21...\n",
            "Extracting DenzelJMD2...\n",
            "Extracting Manuel_Neuer...\n",
            "Extracting ToniRuediger...\n",
            "Extracting MatzeGinter...\n",
            "Extracting matshummels...\n",
            "Extracting kaihavertz29...\n",
            "Extracting ToniKroos...\n",
            "Extracting KeVolland...\n",
            "Extracting SergeGnabry...\n",
            "Extracting Bernd_Leno...\n",
            "Extracting JamalMusiala...\n",
            "Extracting lukaskl96...\n",
            "Extracting leongoretzka_...\n",
            "Extracting leroy_sane...\n",
            "Extracting IlkayGuendogan...\n",
            "Extracting emrecan_...\n",
            "Extracting RobinKoch25...\n",
            "Extracting esmuellert_...\n",
            "Extracting MarshallDavid23...\n",
            "Extracting sodonnell15...\n",
            "Extracting andrewrobertso5...\n",
            "Extracting mctominay10...\n",
            "Extracting granthanley5...\n",
            "Extracting kierantierney1...\n",
            "Extracting jmcginn7...\n",
            "Extracting Callummcgregor8...\n",
            "Extracting Lyndon_Dykes...\n",
            "Extracting CheAdams_...\n",
            "Extracting CraigGordon01...\n",
            "Extracting declang31...\n",
            "Extracting LiamCooper__...\n",
            "Extracting 10DavidTurnbull...\n",
            "Extracting kevinnisbet16...\n",
            "Extracting np4tterson...\n",
            "Extracting billygilmourrr...\n",
            "Extracting Jack_Hendry2...\n",
            "Extracting Scottmckenna3...\n",
            "Extracting BenPavard28...\n",
            "Extracting kimpembe_3...\n",
            "Extracting raphaelvarane...\n",
            "Extracting clement_lenglet...\n",
            "Extracting paulpogba...\n",
            "Extracting AntoGriezmann...\n",
            "Extracting _OlivierGiroud_...\n",
            "Extracting KMbappe...\n",
            "Extracting CorentinTolisso...\n",
            "Extracting nglkante...\n",
            "Extracting KurtZouma...\n",
            "Extracting SteveMandanda...\n",
            "Extracting MoussaSissoko...\n",
            "Extracting LucasDigne...\n",
            "Extracting Benzema...\n",
            "Extracting LucasHernandez...\n",
            "Extracting WissBenYedder...\n",
            "Extracting mmseize...\n",
            "Extracting leodubois15...\n",
            "Extracting jkeey4...\n",
            "Extracting MarcusThuram...\n",
            "Extracting thibautcourtois...\n",
            "Extracting AlderweireldTob...\n",
            "Extracting thomasvermaelen...\n",
            "Extracting JanVertonghen...\n",
            "Extracting axelwitsel28...\n",
            "Extracting DeBruyneKev...\n",
            "Extracting RomeluLukaku9...\n",
            "Extracting hazardeden10...\n",
            "Extracting CarrascoY21...\n",
            "Extracting SMignolet...\n",
            "Extracting dries_mertens14...\n",
            "Extracting ThomMills...\n",
            "Extracting HazardThorgan8...\n",
            "Extracting VanakenHans...\n",
            "Extracting Jasondenayer...\n",
            "Extracting chrisbenteke...\n",
            "Extracting NChadli...\n",
            "Extracting mbatshuayi...\n",
            "Extracting LTrossard...\n",
            "Extracting JeremyDoku...\n",
            "Extracting dennispraet...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oGrxC0CkUj9"
      },
      "source": [
        "# Sort by ascending date\n",
        "joined_df.sort_values('created_at', axis=0, inplace=True)\n",
        "# joined_df['created_at'] = pd.to_datetime(joined_df['created_at'])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsP94a1LlRfo"
      },
      "source": [
        "joined_df = joined_df[\n",
        "    (joined_df[\"JPickford1\"]) |\n",
        "    (joined_df[\"kylewalker2\"]) |\n",
        "    (joined_df[\"LukeShaw23\"]) |\n",
        "    (joined_df[\"kylewalker2\"]) |\n",
        "    (joined_df[\"_DeclanRice\"]) |\n",
        "    (joined_df[\"HarryMaguire93\"]) |\n",
        "    (joined_df[\"JackGrealish\"]) |\n",
        "    (joined_df[\"JHenderson\"]) |\n",
        "    (joined_df[\"HKane\"]) |\n",
        "    (joined_df[\"sterling7\"]) |\n",
        "    (joined_df[\"MarcusRashford\"]) |\n",
        "    (joined_df[\"trippier2\"]) |\n",
        "    (joined_df[\"deanhenderson\"]) |\n",
        "    (joined_df[\"Kalvinphillips\"]) |\n",
        "    (joined_df[\"OfficialTM_3\"]) |\n",
        "    (joined_df[\"Sanchooo10\"]) |\n",
        "    (joined_df[\"CalvertLewin14\"]) |\n",
        "    (joined_df[\"masonmount_10\"]) |\n",
        "    (joined_df[\"PhilFoden\"]) |\n",
        "    (joined_df[\"BenChilwell\"]) |\n",
        "    (joined_df[\"ben6white\"]) |\n",
        "    (joined_df[\"samjohnstone50\"]) |\n",
        "    (joined_df[\"reecejames_24\"]) |\n",
        "    (joined_df[\"BukayoSaka87\"]) |\n",
        "    (joined_df[\"BellinghamJude\"]) |\n",
        "    (joined_df[\"joel_veltman\"]) |\n",
        "    (joined_df[\"mdeligt_04\"]) |\n",
        "    (joined_df[\"LukeShaw23\"]) |\n",
        "    (joined_df[\"NathanAke\"]) |\n",
        "    (joined_df[\"GWijnaldum\"]) |\n",
        "    (joined_df[\"LuukdeJong9\"]) |\n",
        "    (joined_df[\"Memphis\"]) |\n",
        "    (joined_df[\"QPromes\"]) |\n",
        "    (joined_df[\"pvanaanholt\"]) |\n",
        "    (joined_df[\"TimKrul\"]) |\n",
        "    (joined_df[\"DavyKlaassen\"]) |\n",
        "    (joined_df[\"Dirono\"]) |\n",
        "    (joined_df[\"RGravenberch\"]) |\n",
        "    (joined_df[\"BlindDaley\"]) |\n",
        "    (joined_df[\"DeJongFrenkie21\"]) |\n",
        "    (joined_df[\"DenzelJMD2\"]) |\n",
        "    (joined_df[\"Manuel_Neuer\"]) |\n",
        "    (joined_df[\"ToniRuediger\"]) |\n",
        "    (joined_df[\"MatzeGinter\"]) |\n",
        "    (joined_df[\"matshummels\"]) |\n",
        "    (joined_df[\"kaihavertz29\"]) |\n",
        "    (joined_df[\"ToniKroos\"]) |\n",
        "    (joined_df[\"KeVolland\"]) |\n",
        "    (joined_df[\"SergeGnabry\"]) |\n",
        "    (joined_df[\"Bernd_Leno\"]) |\n",
        "    (joined_df[\"JamalMusiala\"]) |\n",
        "    (joined_df[\"lukaskl96\"]) |\n",
        "    (joined_df[\"leongoretzka_\"]) |\n",
        "    (joined_df[\"leroy_sane\"]) |\n",
        "    (joined_df[\"IlkayGuendogan\"]) |\n",
        "    (joined_df[\"emrecan_\"]) |\n",
        "    (joined_df[\"RobinKoch25\"]) |\n",
        "    (joined_df[\"esmuellert_\"]) |\n",
        "    (joined_df[\"MarshallDavid23\"]) |\n",
        "    (joined_df[\"sodonnell15\"]) |\n",
        "    (joined_df[\"andrewrobertso5\"]) |\n",
        "    (joined_df[\"mctominay10\"]) |\n",
        "    (joined_df[\"granthanley5\"]) |\n",
        "    (joined_df[\"kierantierney1\"]) |\n",
        "    (joined_df[\"jmcginn7\"]) |\n",
        "    (joined_df[\"Callummcgregor8\"]) |\n",
        "    (joined_df[\"Lyndon_Dykes\"]) |\n",
        "    (joined_df[\"CheAdams_\"]) |\n",
        "    (joined_df[\"CraigGordon01\"]) |\n",
        "    (joined_df[\"declang31\"]) |\n",
        "    (joined_df[\"LiamCooper__\"]) |\n",
        "    (joined_df[\"10DavidTurnbull\"]) |\n",
        "    (joined_df[\"kevinnisbet16\"]) |\n",
        "    (joined_df[\"np4tterson\"]) |\n",
        "    (joined_df[\"billygilmourrr\"]) |\n",
        "    (joined_df[\"Jack_Hendry2\"]) |\n",
        "    (joined_df[\"Scottmckenna3\"]) |\n",
        "    (joined_df[\"BenPavard28\"]) |\n",
        "    (joined_df[\"kimpembe_3\"]) |\n",
        "    (joined_df[\"raphaelvarane\"]) |\n",
        "    (joined_df[\"clement_lenglet\"]) |\n",
        "    (joined_df[\"paulpogba\"]) |\n",
        "    (joined_df[\"AntoGriezmann\"]) |\n",
        "    (joined_df[\"_OlivierGiroud_\"]) |\n",
        "    (joined_df[\"KMbappe\"]) |\n",
        "    (joined_df[\"CorentinTolisso\"]) |\n",
        "    (joined_df[\"nglkante\"]) |\n",
        "    (joined_df[\"KurtZouma\"]) |\n",
        "    (joined_df[\"SteveMandanda\"]) |\n",
        "    (joined_df[\"MoussaSissoko\"]) |\n",
        "    (joined_df[\"LucasDigne\"]) |\n",
        "    (joined_df[\"Benzema\"]) |\n",
        "    (joined_df[\"LucasHernandez\"]) |\n",
        "    (joined_df[\"WissBenYedder\"]) |\n",
        "    (joined_df[\"mmseize\"]) |\n",
        "    (joined_df[\"leodubois15\"]) |\n",
        "    (joined_df[\"jkeey4\"]) |\n",
        "    (joined_df[\"ben6white\"]) |\n",
        "    (joined_df[\"MarcusThuram\"]) |\n",
        "    (joined_df[\"thibautcourtois\"]) |\n",
        "    (joined_df[\"AlderweireldTob\"]) |\n",
        "    (joined_df[\"thomasvermaelen\"]) |\n",
        "    (joined_df[\"JanVertonghen\"]) |\n",
        "    (joined_df[\"axelwitsel28\"]) |\n",
        "    (joined_df[\"DeBruyneKev\"]) |\n",
        "    (joined_df[\"RomeluLukaku9\"]) |\n",
        "    (joined_df[\"hazardeden10\"]) |\n",
        "    (joined_df[\"CarrascoY21\"]) |\n",
        "    (joined_df[\"SMignolet\"]) |\n",
        "    (joined_df[\"dries_mertens14\"]) |\n",
        "    (joined_df[\"ThomMills\"]) |\n",
        "    (joined_df[\"HazardThorgan8\"]) |\n",
        "    (joined_df[\"VanakenHans\"]) |\n",
        "    (joined_df[\"Jasondenayer\"]) |\n",
        "    (joined_df[\"chrisbenteke\"]) |\n",
        "    (joined_df[\"NChadli\"]) |\n",
        "    (joined_df[\"mbatshuayi\"]) |\n",
        "    (joined_df[\"LTrossard\"]) |\n",
        "    (joined_df[\"JeremyDoku\"]) |\n",
        "    (joined_df[\"dennispraet\"])\n",
        "    ]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLvH1lf747Pq",
        "outputId": "0ed3897b-aeaa-4588-bea0-5b7efe7a8665",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(joined_df)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1046319"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHGk6nctlUPK"
      },
      "source": [
        "# Maybe we don't need these cols and we can just calculate ad-hoc?\n",
        "for player in list_of_players:\n",
        "    joined_df[f'{player}_offensive'] = joined_df['contains_slurs'] & joined_df[player]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZAVdlaR964r"
      },
      "source": [
        "tweets_regression_file = root_path + \"/regression_tweets_hb.csv\"\n",
        "joined_df.to_csv(tweets_regression_file, index=False)"
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}
