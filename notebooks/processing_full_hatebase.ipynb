{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "processing_full_hatebase.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1ypjRKdqqUyS",
        "ngzVB2h4x6J9",
        "oLGzXt3cG1_x"
      ],
      "authorship_tag": "ABX9TyPZZ5qAbb36+6OdSCHdNo6X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbckz/dissertation/blob/master/notebooks/processing_full_hatebase.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br4Ph0_Tpojf"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLv78bpwk10K"
      },
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import plotly.graph_objects as go"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx3CYfwcoyEw",
        "outputId": "cd4f01da-d5b7-430a-c317-78fcf0f6ec4e"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnNZcE26nZ_v"
      },
      "source": [
        "# Set up paths\n",
        "root_path = \"/content/drive/MyDrive/University/Dissertation/data_collection\"\n",
        "graph_path = root_path + \"/graphs\"\n",
        "\n",
        "day_paths = day_paths = [\n",
        "        \"/01\",\n",
        "        \"/02\",\n",
        "        \"/03\",\n",
        "        \"/04\",\n",
        "        \"/05\",\n",
        "        \"/06\",\n",
        "        \"/07\",\n",
        "        \"/08\",\n",
        "        \"/09\",\n",
        "        \"/10\",\n",
        "        \"/11\",\n",
        "        \"/12\",\n",
        "        \"/13\",\n",
        "        \"/14\",\n",
        "        \"/15\",\n",
        "        \"/16\",\n",
        "        \"/17\",\n",
        "        \"/18\",\n",
        "        \"/19\",\n",
        "        \"/20\",\n",
        "        \"/21\",\n",
        "        \"/22\",\n",
        "        \"/23\",\n",
        "        \"/24\",\n",
        "        \"/25\",\n",
        "        \"/26\",\n",
        "        \"/27\",\n",
        "        \"/28\",\n",
        "        \"/29\",\n",
        "        \"/30\",\n",
        "        \"/31\",\n",
        "        \"/32\",\n",
        "        \"/33\",\n",
        "        \"/34\",\n",
        "        \"/35\",\n",
        "        \"/36\"\n",
        "    ]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaaxCbrhnxib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99f7c158-ce01-4f56-b79b-898639cf0e97"
      },
      "source": [
        "# Create directory to store visualisations\n",
        "try:\n",
        "    os.mkdir(graph_path)\n",
        "except OSError as error:\n",
        "    print(error)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 17] File exists: '/content/drive/MyDrive/University/Dissertation/data_collection/graphs'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC_0fxX3oAKF",
        "outputId": "dfcd8732-c2c7-4181-9dc2-b54faa46d51d"
      },
      "source": [
        "# Load data\n",
        "threshold = '90'\n",
        "in_tweets = pd.DataFrame()\n",
        "hb_guard = pd.DataFrame()\n",
        "emojis = pd.DataFrame()\n",
        "for path in day_paths:\n",
        "    directory = root_path + path\n",
        "    tweets_csv = directory + \"/tweets.csv\"\n",
        "    matched_terms_csv = directory + \"/hatebase_processed_tweets.csv\"\n",
        "    emojis_csv = directory + \"/emoji.csv\"\n",
        "\n",
        "    print(f\"Loading CSVs for directory {path}...\")\n",
        "    in_tweets = pd.concat([in_tweets, \n",
        "                           pd.read_csv(tweets_csv,\n",
        "                                       usecols = [\n",
        "                                                  'created_at',\n",
        "                                                  'tweet_id',\n",
        "                                                  'tweet_text',\n",
        "                                                  'accounts_mentioned'\n",
        "                                       ],\n",
        "                                       dtype = {\n",
        "                                          # 'created_at':\n",
        "                                          'tweet_id': np.int64,\n",
        "                                          'tweet_text': str,\n",
        "                                          'accounts_mentioned': object\n",
        "                                       },\n",
        "                                       parse_dates=['created_at'])])\n",
        "\n",
        "    hb_guard = pd.concat([hb_guard, pd.read_csv(matched_terms_csv,\n",
        "                                                usecols = [\n",
        "                                                           'tweet_id',\n",
        "                                                           f'matching_hatebase_terms_over_{threshold}',\n",
        "                                                           f'matching_hatebase_terms_ethnicity_over_{threshold}',\n",
        "                                                           f'matching_hatebase_terms_nationality_over_{threshold}',\n",
        "                                                           f'matching_hatebase_terms_gender_over_{threshold}',\n",
        "                                                           f'matching_hatebase_terms_sexual_orientation_over_{threshold}',\n",
        "                                                           f'matching_hatebase_terms_class_over_{threshold}',\n",
        "                                                           f'matching_hatebase_terms_religion_over_{threshold}',\n",
        "                                                           f'matching_hatebase_terms_disability_over_{threshold}'\n",
        "                                                ],\n",
        "                                                dtype = {\n",
        "                                                    'tweet_id': np.int64,\n",
        "                                                    f'matching_hatebase_terms_over_{threshold}': str,\n",
        "                                                    f'matching_hatebase_terms_ethnicity_over_{threshold}': str,\n",
        "                                                    f'matching_hatebase_terms_nationality_over_{threshold}': str,\n",
        "                                                    f'matching_hatebase_terms_gender_over_{threshold}': str,\n",
        "                                                    f'matching_hatebase_terms_sexual_orientation_over_{threshold}': str,\n",
        "                                                    f'matching_hatebase_terms_class_over_{threshold}': str,\n",
        "                                                    f'matching_hatebase_terms_religion_over_{threshold}': str,\n",
        "                                                    f'matching_hatebase_terms_disability_over_{threshold}': str\n",
        "                                                })])\n",
        "\n",
        "    emojis = pd.concat([emojis,\n",
        "                        pd.read_csv(emojis_csv,\n",
        "                                    dtype = {\n",
        "                                        'tweet_id': np.int64,\n",
        "                                        'banana_count': np.int16,\n",
        "                                        'monkey_count': np.int16,\n",
        "                                        'monkey_face_count': np.int16,\n",
        "                                        'speak_no_evil_monkey_count': np.int16,\n",
        "                                        'hear_no_evil_monkey_count': np.int16,\n",
        "                                        'see_no_evil_monkey_count': np.int16,\n",
        "                                        'gorilla_count': np.int16,\n",
        "                                        'watermelon_count': np.int16,\n",
        "                                        'total_emoji_count': np.int16\n",
        "                                    }\n",
        "                                    )])\n",
        "\n",
        "# Dedup\n",
        "original_tweets_length = len(in_tweets)\n",
        "original_hatebase_length = len(hb_guard)\n",
        "original_emojis_length = len(emojis)\n",
        "in_tweets.drop_duplicates(subset=['tweet_id'], inplace=True)\n",
        "hb_guard.drop_duplicates(subset=['tweet_id'], inplace=True)\n",
        "emojis.drop_duplicates(subset=['tweet_id'], inplace=True)\n",
        "print(f\"Size of tweets dataframe: {len(in_tweets)}, having dropped {original_tweets_length - len(in_tweets)} duplicate rows\")\n",
        "print(f\"Size of hatebase dataframe: {len(hb_guard)}, having dropped {original_hatebase_length - len(hb_guard)} duplicate rows\")\n",
        "print(f\"Size of emojis dataframe: {len(emojis)}, having dropped {original_emojis_length - len(emojis)} duplicate rows\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading CSVs for directory /01...\n",
            "Loading CSVs for directory /02...\n",
            "Loading CSVs for directory /03...\n",
            "Loading CSVs for directory /04...\n",
            "Loading CSVs for directory /05...\n",
            "Loading CSVs for directory /06...\n",
            "Loading CSVs for directory /07...\n",
            "Loading CSVs for directory /08...\n",
            "Loading CSVs for directory /09...\n",
            "Loading CSVs for directory /10...\n",
            "Loading CSVs for directory /11...\n",
            "Loading CSVs for directory /12...\n",
            "Loading CSVs for directory /13...\n",
            "Loading CSVs for directory /14...\n",
            "Loading CSVs for directory /15...\n",
            "Loading CSVs for directory /16...\n",
            "Loading CSVs for directory /17...\n",
            "Loading CSVs for directory /18...\n",
            "Loading CSVs for directory /19...\n",
            "Loading CSVs for directory /20...\n",
            "Loading CSVs for directory /21...\n",
            "Loading CSVs for directory /22...\n",
            "Loading CSVs for directory /23...\n",
            "Loading CSVs for directory /24...\n",
            "Loading CSVs for directory /25...\n",
            "Loading CSVs for directory /26...\n",
            "Loading CSVs for directory /27...\n",
            "Loading CSVs for directory /28...\n",
            "Loading CSVs for directory /29...\n",
            "Loading CSVs for directory /30...\n",
            "Loading CSVs for directory /31...\n",
            "Loading CSVs for directory /32...\n",
            "Loading CSVs for directory /33...\n",
            "Loading CSVs for directory /34...\n",
            "Loading CSVs for directory /35...\n",
            "Loading CSVs for directory /36...\n",
            "Size of tweets dataframe: 1478009, having dropped 133 duplicate rows\n",
            "Size of hatebase dataframe: 1478009, having dropped 133 duplicate rows\n",
            "Size of emojis dataframe: 1478009, having dropped 133 duplicate rows\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIQbp7k92PQ_"
      },
      "source": [
        "# Up the pandas display limits so printed dataframes aren't so truncated\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_info_rows', 100)\n",
        "pd.set_option('display.max_info_columns', 100)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKwJiJz_pvQ9"
      },
      "source": [
        "# Data manipulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHz_MRv9p1P9"
      },
      "source": [
        "# Join tables + drop old ones!\n",
        "joined_df = pd.merge(in_tweets, emojis, how='outer', on='tweet_id')\n",
        "joined_df = pd.merge(joined_df, hb_guard, how='outer', on='tweet_id')\n",
        "del emojis\n",
        "del hb_guard\n",
        "del in_tweets"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCWstScwta0g",
        "outputId": "4add1c04-1e63-4c82-fedb-c4b418c42ede"
      },
      "source": [
        "joined_df = joined_df[(joined_df['created_at'] > '2021-06-19 08:10:18+00:00') & (joined_df['created_at'] < '2021-07-17 00:00:00+00:00')]\n",
        "len(joined_df)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1274885"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpfkGRwSqBzL"
      },
      "source": [
        "# Create a column indicating whether tweet contains slurs (hacky > 2 as empty list is stored as string \"[]\")\n",
        "joined_df['contains_slurs'] = joined_df[f'matching_hatebase_terms_over_{threshold}'].str.len() > 2\n",
        "joined_df['contains_ethnicity_slurs'] = joined_df[f'matching_hatebase_terms_ethnicity_over_{threshold}'].str.len() > 2\n",
        "joined_df['contains_nationality_slurs'] = joined_df[f'matching_hatebase_terms_nationality_over_{threshold}'].str.len() > 2\n",
        "joined_df['contains_gender_slurs'] = joined_df[f'matching_hatebase_terms_gender_over_{threshold}'].str.len() > 2\n",
        "joined_df['contains_sexual_orientation_slurs'] = joined_df[f'matching_hatebase_terms_sexual_orientation_over_{threshold}'].str.len() > 2\n",
        "joined_df['contains_class_slurs'] = joined_df[f'matching_hatebase_terms_class_over_{threshold}'].str.len() > 2\n",
        "joined_df['contains_religion_slurs'] = joined_df[f'matching_hatebase_terms_religion_over_{threshold}'].str.len() > 2\n",
        "joined_df['contains_disability_slurs'] = joined_df[f'matching_hatebase_terms_disability_over_{threshold}'].str.len() > 2\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb0cK5Rixe9r"
      },
      "source": [
        "emoji_tweet_file = root_path + \"/emoji_tweets.csv\"\n",
        "\n",
        "banana_tweet_count = joined_df['banana_count'][joined_df['banana_count'] != 0].count()\n",
        "monkey_tweet_count = joined_df['monkey_count'][joined_df['banana_count'] != 0].count()\n",
        "monkey_face_tweet_count = joined_df['monkey_face_count'][joined_df['monkey_face_count'] != 0].count()\n",
        "speak_no_evil_monkey_tweet_count = joined_df['speak_no_evil_monkey_count'][joined_df['speak_no_evil_monkey_count'] != 0].count()\n",
        "hear_no_evil_monkey_tweet_count = joined_df['hear_no_evil_monkey_count'][joined_df['hear_no_evil_monkey_count'] != 0].count()\n",
        "see_no_evil_monkey_tweet_count = joined_df['see_no_evil_monkey_count'][joined_df['see_no_evil_monkey_count'] != 0].count()\n",
        "gorilla_tweet_count = joined_df['gorilla_count'][joined_df['gorilla_count'] != 0].count()\n",
        "watermelon_tweet_count = joined_df['watermelon_count'][joined_df['watermelon_count'] != 0].count()\n",
        "\n",
        "banana_total = joined_df['banana_count'][joined_df['banana_count'] != 0].sum()\n",
        "monkey_total = joined_df['monkey_count'][joined_df['monkey_count'] != 0].sum()\n",
        "monkey_face_total = joined_df['monkey_face_count'][joined_df['monkey_face_count'] != 0].sum()\n",
        "speak_no_evil_monkey_total = joined_df['speak_no_evil_monkey_count'][joined_df['speak_no_evil_monkey_count'] != 0].sum()\n",
        "hear_no_evil_monkey_total = joined_df['hear_no_evil_monkey_count'][joined_df['hear_no_evil_monkey_count'] != 0].sum()\n",
        "see_no_evil_monkey_total = joined_df['see_no_evil_monkey_count'][joined_df['see_no_evil_monkey_count'] != 0].sum()\n",
        "gorilla_total = joined_df['gorilla_count'][joined_df['gorilla_count'] != 0].sum()\n",
        "watermelon_total = joined_df['watermelon_count'][joined_df['watermelon_count'] != 0].sum()\n",
        "\n",
        "total_emoji_tweets = joined_df.query('banana_count != 0 or monkey_count != 0 or monkey_face_count != 0 or speak_no_evil_monkey_count != 0 or hear_no_evil_monkey_count != 0 or see_no_evil_monkey_count != 0 or gorilla_count != 0 or watermelon_count != 0')['tweet_id'].count()\n",
        "total_emoji_count = joined_df['total_emoji_count'][joined_df['total_emoji_count'] != 0].sum()\n",
        "\n",
        "joined_df[['tweet_id', 'tweet_text']][joined_df['total_emoji_count'] > 0].to_csv(emoji_tweet_file, index=False)\n",
        "joined_df['contains_emoji_slurs'] = joined_df['total_emoji_count'] > 0"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoLsN1wGxkWg"
      },
      "source": [
        "# NOTE: This shouldn't go here - this should be part of the evaluation process\n",
        "\n",
        "# # NOTE: at this point you need to manually review the emoji tweets in emoji_tweets.csv, and put them into a emoji_tweets_reviewed.csv\n",
        "\n",
        "# reviewed_emojis = pd.read_csv(root_path + '/emoji_tweets_reviewed.csv')\n",
        "# reviewed_emojis['manually_reviewed_emoji_is_offensive'] = True\n",
        "# reviewed_emojis.drop('tweet_text', axis=1, inplace=True)\n",
        "# joined_df = pd.merge(joined_df, reviewed_emojis, how='outer', on='tweet_id')\n",
        "# joined_df['manually_reviewed_emoji_is_offensive'] = joined_df['manually_reviewed_emoji_is_offensive'].fillna(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2wlAG3DvVEC",
        "outputId": "3c162ea0-01eb-461e-be14-bca2f164b235"
      },
      "source": [
        "# Extract players\n",
        "england = [\"JPickford1\", \"kylewalker2\", \"LukeShaw23\", \"_DeclanRice\", \"HarryMaguire93\", \"JackGrealish\",\n",
        "                    \"JHenderson\", \"HKane\", \"sterling7\", \"MarcusRashford\", \"trippier2\", \"deanhenderson\",\n",
        "                    \"Kalvinphillips\", \"OfficialTM_3\", \"Sanchooo10\", \"CalvertLewin14\", \"masonmount_10\", \"PhilFoden\",\n",
        "                    \"BenChilwell\", \"ben6white\", \"samjohnstone50\", \"reecejames_24\", \"BukayoSaka87\", \"BellinghamJude\"]\n",
        "\n",
        "netherlands = [\"joel_veltman\", \"mdeligt_04\", \"NathanAke\", \"Stefandevrij\", \"GWijnaldum\", \"LuukdeJong9\", \"Memphis\", \"QPromes\", \"pvanaanholt\", \"TimKrul\", \"DavyKlaassen\", \"Dirono\", \"RGravenberch\", \"BlindDaley\", \"DeJongFrenkie21\", \"DenzelJMD2\"]\n",
        "\n",
        "germany = [\"Manuel_Neuer\", \"ToniRuediger\", \"MatzeGinter\", \"matshummels\", \"kaihavertz29\", \"ToniKroos\", \"KeVolland\", \"SergeGnabry\", \"Bernd_Leno\", \"JamalMusiala\", \"lukaskl96\", \"leongoretzka_\", \"leroy_sane\", \"IlkayGuendogan\", \"emrecan_\", \"RobinKoch25\", \"esmuellert_\"]\n",
        "\n",
        "scotland = [\"MarshallDavid23\", \"sodonnell15\", \"andrewrobertso5\", \"mctominay10\", \"granthanley5\", \"kierantierney1\", \"jmcginn7\", \"Callummcgregor8\", \"Lyndon_Dykes\", \"CheAdams_\", \"CraigGordon01\", \"declang31\", \"LiamCooper__\", \"10DavidTurnbull\", \"kevinnisbet16\", \"np4tterson\", \"billygilmourrr\", \"Jack_Hendry2\", \"Scottmckenna3\"]\n",
        "\n",
        "france = [\"BenPavard28\", \"kimpembe_3\", \"raphaelvarane\", \"clement_lenglet\", \"paulpogba\", \"AntoGriezmann\", \"_OlivierGiroud_\", \"KMbappe\", \"CorentinTolisso\", \"nglkante\", \"KurtZouma\", \"SteveMandanda\", \"MoussaSissoko\", \"LucasDigne\", \"Benzema\", \"LucasHernandez\", \"WissBenYedder\", \"mmseize\", \"leodubois15\", \"jkeey4\", \"MarcusThuram\"]\n",
        "\n",
        "belgium = [\"thibautcourtois\", \"AlderweireldTob\", \"thomasvermaelen\", \"JanVertonghen\", \"axelwitsel28\", \"DeBruyneKev\", \"RomeluLukaku9\", \"hazardeden10\", \"CarrascoY21\", \"SMignolet\", \"dries_mertens14\", \"ThomMills\", \"HazardThorgan8\", \"VanakenHans\", \"Jasondenayer\", \"chrisbenteke\", \"NChadli\", \"mbatshuayi\", \"LTrossard\", \"JeremyDoku\", \"dennispraet\"]\n",
        "\n",
        "list_of_players = england + netherlands + germany + scotland + france + belgium\n",
        "\n",
        "for player in list_of_players:\n",
        "    print(f\"Extracting {player}...\")\n",
        "    joined_df[player] = joined_df['accounts_mentioned'].str.contains(f\"'username': '{player}'\").astype(np.bool)\n",
        "\n",
        "# player_tweet_map = pd.DataFrame(columns=[\"username\", \"tweets_received\"])\n",
        "\n",
        "# i = 0\n",
        "# for player in list_of_players:\n",
        "#     tweets = joined_df[player].sum()\n",
        "#     player_tweet_map.loc[i] = player, tweets\n",
        "#     i += 1\n",
        "\n",
        "# player_tweet_map.sort_values('tweets_received', axis=0, ascending=False, inplace=True)\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting JPickford1...\n",
            "Extracting kylewalker2...\n",
            "Extracting LukeShaw23...\n",
            "Extracting _DeclanRice...\n",
            "Extracting HarryMaguire93...\n",
            "Extracting JackGrealish...\n",
            "Extracting JHenderson...\n",
            "Extracting HKane...\n",
            "Extracting sterling7...\n",
            "Extracting MarcusRashford...\n",
            "Extracting trippier2...\n",
            "Extracting deanhenderson...\n",
            "Extracting Kalvinphillips...\n",
            "Extracting OfficialTM_3...\n",
            "Extracting Sanchooo10...\n",
            "Extracting CalvertLewin14...\n",
            "Extracting masonmount_10...\n",
            "Extracting PhilFoden...\n",
            "Extracting BenChilwell...\n",
            "Extracting ben6white...\n",
            "Extracting samjohnstone50...\n",
            "Extracting reecejames_24...\n",
            "Extracting BukayoSaka87...\n",
            "Extracting BellinghamJude...\n",
            "Extracting joel_veltman...\n",
            "Extracting mdeligt_04...\n",
            "Extracting NathanAke...\n",
            "Extracting Stefandevrij...\n",
            "Extracting GWijnaldum...\n",
            "Extracting LuukdeJong9...\n",
            "Extracting Memphis...\n",
            "Extracting QPromes...\n",
            "Extracting pvanaanholt...\n",
            "Extracting TimKrul...\n",
            "Extracting DavyKlaassen...\n",
            "Extracting Dirono...\n",
            "Extracting RGravenberch...\n",
            "Extracting BlindDaley...\n",
            "Extracting DeJongFrenkie21...\n",
            "Extracting DenzelJMD2...\n",
            "Extracting Manuel_Neuer...\n",
            "Extracting ToniRuediger...\n",
            "Extracting MatzeGinter...\n",
            "Extracting matshummels...\n",
            "Extracting kaihavertz29...\n",
            "Extracting ToniKroos...\n",
            "Extracting KeVolland...\n",
            "Extracting SergeGnabry...\n",
            "Extracting Bernd_Leno...\n",
            "Extracting JamalMusiala...\n",
            "Extracting lukaskl96...\n",
            "Extracting leongoretzka_...\n",
            "Extracting leroy_sane...\n",
            "Extracting IlkayGuendogan...\n",
            "Extracting emrecan_...\n",
            "Extracting RobinKoch25...\n",
            "Extracting esmuellert_...\n",
            "Extracting MarshallDavid23...\n",
            "Extracting sodonnell15...\n",
            "Extracting andrewrobertso5...\n",
            "Extracting mctominay10...\n",
            "Extracting granthanley5...\n",
            "Extracting kierantierney1...\n",
            "Extracting jmcginn7...\n",
            "Extracting Callummcgregor8...\n",
            "Extracting Lyndon_Dykes...\n",
            "Extracting CheAdams_...\n",
            "Extracting CraigGordon01...\n",
            "Extracting declang31...\n",
            "Extracting LiamCooper__...\n",
            "Extracting 10DavidTurnbull...\n",
            "Extracting kevinnisbet16...\n",
            "Extracting np4tterson...\n",
            "Extracting billygilmourrr...\n",
            "Extracting Jack_Hendry2...\n",
            "Extracting Scottmckenna3...\n",
            "Extracting BenPavard28...\n",
            "Extracting kimpembe_3...\n",
            "Extracting raphaelvarane...\n",
            "Extracting clement_lenglet...\n",
            "Extracting paulpogba...\n",
            "Extracting AntoGriezmann...\n",
            "Extracting _OlivierGiroud_...\n",
            "Extracting KMbappe...\n",
            "Extracting CorentinTolisso...\n",
            "Extracting nglkante...\n",
            "Extracting KurtZouma...\n",
            "Extracting SteveMandanda...\n",
            "Extracting MoussaSissoko...\n",
            "Extracting LucasDigne...\n",
            "Extracting Benzema...\n",
            "Extracting LucasHernandez...\n",
            "Extracting WissBenYedder...\n",
            "Extracting mmseize...\n",
            "Extracting leodubois15...\n",
            "Extracting jkeey4...\n",
            "Extracting MarcusThuram...\n",
            "Extracting thibautcourtois...\n",
            "Extracting AlderweireldTob...\n",
            "Extracting thomasvermaelen...\n",
            "Extracting JanVertonghen...\n",
            "Extracting axelwitsel28...\n",
            "Extracting DeBruyneKev...\n",
            "Extracting RomeluLukaku9...\n",
            "Extracting hazardeden10...\n",
            "Extracting CarrascoY21...\n",
            "Extracting SMignolet...\n",
            "Extracting dries_mertens14...\n",
            "Extracting ThomMills...\n",
            "Extracting HazardThorgan8...\n",
            "Extracting VanakenHans...\n",
            "Extracting Jasondenayer...\n",
            "Extracting chrisbenteke...\n",
            "Extracting NChadli...\n",
            "Extracting mbatshuayi...\n",
            "Extracting LTrossard...\n",
            "Extracting JeremyDoku...\n",
            "Extracting dennispraet...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oGrxC0CkUj9"
      },
      "source": [
        "# Sort by ascending date\n",
        "joined_df.sort_values('created_at', axis=0, inplace=True)\n",
        "# joined_df['created_at'] = pd.to_datetime(joined_df['created_at'])\n",
        "\n",
        "joined_df['contains_slurs_or_offensive_emoji'] = joined_df['contains_slurs'] | joined_df['contains_emoji_slurs']\n",
        "# joined_df['contains_ethnicity_slurs'] = joined_df['contains_ethnicity_slurs'] | joined_df['manually_reviewed_emoji_is_offensive'] # assuming all emoji ones are racist"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsP94a1LlRfo"
      },
      "source": [
        "joined_df = joined_df[\n",
        "    (joined_df[\"JPickford1\"]) |\n",
        "    (joined_df[\"kylewalker2\"]) |\n",
        "    (joined_df[\"LukeShaw23\"]) |\n",
        "    (joined_df[\"kylewalker2\"]) |\n",
        "    (joined_df[\"_DeclanRice\"]) |\n",
        "    (joined_df[\"HarryMaguire93\"]) |\n",
        "    (joined_df[\"JackGrealish\"]) |\n",
        "    (joined_df[\"JHenderson\"]) |\n",
        "    (joined_df[\"HKane\"]) |\n",
        "    (joined_df[\"sterling7\"]) |\n",
        "    (joined_df[\"MarcusRashford\"]) |\n",
        "    (joined_df[\"trippier2\"]) |\n",
        "    (joined_df[\"deanhenderson\"]) |\n",
        "    (joined_df[\"Kalvinphillips\"]) |\n",
        "    (joined_df[\"OfficialTM_3\"]) |\n",
        "    (joined_df[\"Sanchooo10\"]) |\n",
        "    (joined_df[\"CalvertLewin14\"]) |\n",
        "    (joined_df[\"masonmount_10\"]) |\n",
        "    (joined_df[\"PhilFoden\"]) |\n",
        "    (joined_df[\"BenChilwell\"]) |\n",
        "    (joined_df[\"ben6white\"]) |\n",
        "    (joined_df[\"samjohnstone50\"]) |\n",
        "    (joined_df[\"reecejames_24\"]) |\n",
        "    (joined_df[\"BukayoSaka87\"]) |\n",
        "    (joined_df[\"BellinghamJude\"]) |\n",
        "    (joined_df[\"joel_veltman\"]) |\n",
        "    (joined_df[\"mdeligt_04\"]) |\n",
        "    (joined_df[\"LukeShaw23\"]) |\n",
        "    (joined_df[\"NathanAke\"]) |\n",
        "    (joined_df[\"GWijnaldum\"]) |\n",
        "    (joined_df[\"LuukdeJong9\"]) |\n",
        "    (joined_df[\"Memphis\"]) |\n",
        "    (joined_df[\"QPromes\"]) |\n",
        "    (joined_df[\"pvanaanholt\"]) |\n",
        "    (joined_df[\"TimKrul\"]) |\n",
        "    (joined_df[\"DavyKlaassen\"]) |\n",
        "    (joined_df[\"Dirono\"]) |\n",
        "    (joined_df[\"RGravenberch\"]) |\n",
        "    (joined_df[\"BlindDaley\"]) |\n",
        "    (joined_df[\"DeJongFrenkie21\"]) |\n",
        "    (joined_df[\"DenzelJMD2\"]) |\n",
        "    (joined_df[\"Manuel_Neuer\"]) |\n",
        "    (joined_df[\"ToniRuediger\"]) |\n",
        "    (joined_df[\"MatzeGinter\"]) |\n",
        "    (joined_df[\"matshummels\"]) |\n",
        "    (joined_df[\"kaihavertz29\"]) |\n",
        "    (joined_df[\"ToniKroos\"]) |\n",
        "    (joined_df[\"KeVolland\"]) |\n",
        "    (joined_df[\"SergeGnabry\"]) |\n",
        "    (joined_df[\"Bernd_Leno\"]) |\n",
        "    (joined_df[\"JamalMusiala\"]) |\n",
        "    (joined_df[\"lukaskl96\"]) |\n",
        "    (joined_df[\"leongoretzka_\"]) |\n",
        "    (joined_df[\"leroy_sane\"]) |\n",
        "    (joined_df[\"IlkayGuendogan\"]) |\n",
        "    (joined_df[\"emrecan_\"]) |\n",
        "    (joined_df[\"RobinKoch25\"]) |\n",
        "    (joined_df[\"esmuellert_\"]) |\n",
        "    (joined_df[\"MarshallDavid23\"]) |\n",
        "    (joined_df[\"sodonnell15\"]) |\n",
        "    (joined_df[\"andrewrobertso5\"]) |\n",
        "    (joined_df[\"mctominay10\"]) |\n",
        "    (joined_df[\"granthanley5\"]) |\n",
        "    (joined_df[\"kierantierney1\"]) |\n",
        "    (joined_df[\"jmcginn7\"]) |\n",
        "    (joined_df[\"Callummcgregor8\"]) |\n",
        "    (joined_df[\"Lyndon_Dykes\"]) |\n",
        "    (joined_df[\"CheAdams_\"]) |\n",
        "    (joined_df[\"CraigGordon01\"]) |\n",
        "    (joined_df[\"declang31\"]) |\n",
        "    (joined_df[\"LiamCooper__\"]) |\n",
        "    (joined_df[\"10DavidTurnbull\"]) |\n",
        "    (joined_df[\"kevinnisbet16\"]) |\n",
        "    (joined_df[\"np4tterson\"]) |\n",
        "    (joined_df[\"billygilmourrr\"]) |\n",
        "    (joined_df[\"Jack_Hendry2\"]) |\n",
        "    (joined_df[\"Scottmckenna3\"]) |\n",
        "    (joined_df[\"BenPavard28\"]) |\n",
        "    (joined_df[\"kimpembe_3\"]) |\n",
        "    (joined_df[\"raphaelvarane\"]) |\n",
        "    (joined_df[\"clement_lenglet\"]) |\n",
        "    (joined_df[\"paulpogba\"]) |\n",
        "    (joined_df[\"AntoGriezmann\"]) |\n",
        "    (joined_df[\"_OlivierGiroud_\"]) |\n",
        "    (joined_df[\"KMbappe\"]) |\n",
        "    (joined_df[\"CorentinTolisso\"]) |\n",
        "    (joined_df[\"nglkante\"]) |\n",
        "    (joined_df[\"KurtZouma\"]) |\n",
        "    (joined_df[\"SteveMandanda\"]) |\n",
        "    (joined_df[\"MoussaSissoko\"]) |\n",
        "    (joined_df[\"LucasDigne\"]) |\n",
        "    (joined_df[\"Benzema\"]) |\n",
        "    (joined_df[\"LucasHernandez\"]) |\n",
        "    (joined_df[\"WissBenYedder\"]) |\n",
        "    (joined_df[\"mmseize\"]) |\n",
        "    (joined_df[\"leodubois15\"]) |\n",
        "    (joined_df[\"jkeey4\"]) |\n",
        "    (joined_df[\"ben6white\"]) |\n",
        "    (joined_df[\"MarcusThuram\"]) |\n",
        "    (joined_df[\"thibautcourtois\"]) |\n",
        "    (joined_df[\"AlderweireldTob\"]) |\n",
        "    (joined_df[\"thomasvermaelen\"]) |\n",
        "    (joined_df[\"JanVertonghen\"]) |\n",
        "    (joined_df[\"axelwitsel28\"]) |\n",
        "    (joined_df[\"DeBruyneKev\"]) |\n",
        "    (joined_df[\"RomeluLukaku9\"]) |\n",
        "    (joined_df[\"hazardeden10\"]) |\n",
        "    (joined_df[\"CarrascoY21\"]) |\n",
        "    (joined_df[\"SMignolet\"]) |\n",
        "    (joined_df[\"dries_mertens14\"]) |\n",
        "    (joined_df[\"ThomMills\"]) |\n",
        "    (joined_df[\"HazardThorgan8\"]) |\n",
        "    (joined_df[\"VanakenHans\"]) |\n",
        "    (joined_df[\"Jasondenayer\"]) |\n",
        "    (joined_df[\"chrisbenteke\"]) |\n",
        "    (joined_df[\"NChadli\"]) |\n",
        "    (joined_df[\"mbatshuayi\"]) |\n",
        "    (joined_df[\"LTrossard\"]) |\n",
        "    (joined_df[\"JeremyDoku\"]) |\n",
        "    (joined_df[\"dennispraet\"])\n",
        "    ]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4vnCbPHv2-O",
        "outputId": "03d24ab5-3654-4f56-ad8b-4a0097a7eef8"
      },
      "source": [
        "total_off_tweets = joined_df['contains_slurs_or_offensive_emoji'].sum()\n",
        "total_tweets = len(joined_df)\n",
        "print(f\"Total tweets containing slurs: {total_off_tweets}\")\n",
        "print(f\"Total tweets: {total_tweets}\")\n",
        "print(f\"Percentage of tweets containing slurs: {(100*total_off_tweets)/total_tweets}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total tweets containing slurs: 4970\n",
            "Total tweets: 1046319\n",
            "Percentage of tweets containing slurs: 0.47499854250950235\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWg567XY9suk"
      },
      "source": [
        "# Saving a file for manual review, taken at threshold 90 - one-off ad-hoc task\n",
        "# joined_df[['tweet_id', 'tweet_text']][joined_df['contains_slurs_or_offensive_emoji'] == True].to_csv(\"/content/drive/MyDrive/University/Dissertation/evaluation/tweets_hb.csv\", index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHGk6nctlUPK"
      },
      "source": [
        "# Maybe we don't need these cols and we can just calculate ad-hoc?\n",
        "for player in list_of_players:\n",
        "    joined_df[f'{player}_offensive'] = joined_df['contains_slurs_or_offensive_emoji'] & joined_df[player]\n",
        "    # joined_df[f'{player}_ethnicity'] = joined_df['contains_ethnicity_slurs'] & joined_df[player]\n",
        "    # joined_df[f'{player}_nationality'] = joined_df['contains_nationality_slurs'] & joined_df[player]\n",
        "    # joined_df[f'{player}_gender'] = joined_df['contains_gender_slurs'] & joined_df[player]\n",
        "    # joined_df[f'{player}_sexual_orientation'] = joined_df['contains_sexual_orientation_slurs'] & joined_df[player]\n",
        "    # joined_df[f'{player}_class'] = joined_df['contains_class_slurs'] & joined_df[player]\n",
        "    # joined_df[f'{player}_religion'] = joined_df['contains_religion_slurs'] & joined_df[player]\n",
        "    # joined_df[f'{player}_disability'] = joined_df['contains_disability_slurs'] & joined_df[player]\n",
        "    # joined_df[f'{player}_emoji'] = joined_df['contains_emoji_slurs'] & joined_df[player]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJhEU6wE2a8b",
        "outputId": "0db3dfc9-4480-459f-c6c7-d17e506d73a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "player_offensive_tweet_map = pd.DataFrame(columns=[\"username\",\n",
        "                                                   \"tweets_received\",\n",
        "                                                   \"offensive_tweets_received\",\n",
        "                                                  #  \"ethnicity_tweets_received\",\n",
        "                                                  #  \"nationality_tweets_received\",\n",
        "                                                  #  \"gender_tweets_received\",\n",
        "                                                  #  \"sexual_orientation_tweets_received\",\n",
        "                                                  #  \"class_tweets_received\",\n",
        "                                                  #  \"religion_tweets_received\",\n",
        "                                                  #  \"disability_tweets_received\",\n",
        "                                                  #  \"emoji_tweets_received\",\n",
        "                                                   \"percentage_offensive\"\n",
        "                                                   ])\n",
        "\n",
        "i = 0\n",
        "for player in list_of_players:\n",
        "    off_tweets = joined_df[player + '_offensive'].sum()\n",
        "    # ethnicity_tweets = joined_df[player + '_ethnicity'].sum()\n",
        "    # nationality_tweets = joined_df[player + '_nationality'].sum()\n",
        "    # gender_tweets = joined_df[player + '_gender'].sum()\n",
        "    # sexual_orientation_tweets = joined_df[player + '_sexual_orientation'].sum()\n",
        "    # class_tweets = joined_df[player + '_class'].sum()\n",
        "    # religion_tweets = joined_df[player + '_religion'].sum()\n",
        "    # disability_tweets = joined_df[player + '_disability'].sum()\n",
        "    # emoji_tweets = joined_df[player + '_emoji'].sum()\n",
        "    tweets = joined_df[player].sum()\n",
        "    percentage = 100 * (off_tweets / tweets)\n",
        "    # player_offensive_tweet_map.loc[i] = player, tweets, off_tweets, ethnicity_tweets, nationality_tweets, gender_tweets, sexual_orientation_tweets, class_tweets, religion_tweets, disability_tweets, emoji_tweets, percentage\n",
        "    player_offensive_tweet_map.loc[i] = player, tweets, off_tweets, percentage\n",
        "    i += 1"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in long_scalars\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmeSSXawP0Qb"
      },
      "source": [
        "# # Create per country map\n",
        "# country_offensive_tweet_map = pd.DataFrame(columns=[\"country\",\n",
        "#                                                    \"tweets_received\",\n",
        "#                                                    \"offensive_tweets_received\",\n",
        "#                                                    \"ethnicity_tweets_received\",\n",
        "#                                                    \"nationality_tweets_received\",\n",
        "#                                                    \"gender_tweets_received\",\n",
        "#                                                    \"sexual_orientation_tweets_received\",\n",
        "#                                                    \"class_tweets_received\",\n",
        "#                                                    \"religion_tweets_received\",\n",
        "#                                                    \"disability_tweets_received\",\n",
        "#                                                    \"emoji_tweets_received\",\n",
        "#                                                    \"percentage_offensive\"\n",
        "#                                                   ])\n",
        "\n",
        "# country_offensive_tweet_map.loc[0] = \"england\", \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(england)]['tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(england)]['offensive_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(england)]['ethnicity_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(england)]['nationality_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(england)]['gender_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(england)]['sexual_orientation_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(england)]['class_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(england)]['religion_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(england)]['disability_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(england)]['emoji_tweets_received'].sum(), \\\n",
        "#                                      0\n",
        "                                     \n",
        "# country_offensive_tweet_map.loc[0]['percentage_offensive'] = 100 * (country_offensive_tweet_map.loc[0]['offensive_tweets_received'] / country_offensive_tweet_map.loc[0]['tweets_received'])\n",
        "\n",
        "# country_offensive_tweet_map.loc[1] = \"netherlands\", \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(netherlands)]['tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(netherlands)]['offensive_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(netherlands)]['ethnicity_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(netherlands)]['nationality_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(netherlands)]['gender_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(netherlands)]['sexual_orientation_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(netherlands)]['class_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(netherlands)]['religion_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(netherlands)]['disability_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(england)]['emoji_tweets_received'].sum(), \\\n",
        "#                                      0\n",
        "                                     \n",
        "# country_offensive_tweet_map.loc[1]['percentage_offensive'] = 100 * (country_offensive_tweet_map.loc[1]['offensive_tweets_received'] / country_offensive_tweet_map.loc[1]['tweets_received'])\n",
        "\n",
        "# country_offensive_tweet_map.loc[2] = \"germany\", \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(germany)]['tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(germany)]['offensive_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(germany)]['ethnicity_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(germany)]['nationality_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(germany)]['gender_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(germany)]['sexual_orientation_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(germany)]['class_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(germany)]['religion_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(germany)]['disability_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(england)]['emoji_tweets_received'].sum(), \\\n",
        "#                                      0\n",
        "                                     \n",
        "# country_offensive_tweet_map.loc[2]['percentage_offensive'] = 100 * (country_offensive_tweet_map.loc[2]['offensive_tweets_received'] / country_offensive_tweet_map.loc[2]['tweets_received'])\n",
        "\n",
        "# country_offensive_tweet_map.loc[3] = \"france\", \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(france)]['tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(france)]['offensive_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(france)]['ethnicity_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(france)]['nationality_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(france)]['gender_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(france)]['sexual_orientation_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(france)]['class_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(france)]['religion_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(france)]['disability_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(england)]['emoji_tweets_received'].sum(), \\\n",
        "#                                      0\n",
        "                                     \n",
        "# country_offensive_tweet_map.loc[3]['percentage_offensive'] = 100 * (country_offensive_tweet_map.loc[3]['offensive_tweets_received'] / country_offensive_tweet_map.loc[3]['tweets_received'])\n",
        "\n",
        "# country_offensive_tweet_map.loc[4] = \"scotland\", \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(scotland)]['tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(scotland)]['offensive_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(scotland)]['ethnicity_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(scotland)]['nationality_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(scotland)]['gender_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(scotland)]['sexual_orientation_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(scotland)]['class_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(scotland)]['religion_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(scotland)]['disability_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(england)]['emoji_tweets_received'].sum(), \\\n",
        "#                                      0\n",
        "                                     \n",
        "# country_offensive_tweet_map.loc[4]['percentage_offensive'] = 100 * (country_offensive_tweet_map.loc[4]['offensive_tweets_received'] / country_offensive_tweet_map.loc[4]['tweets_received'])\n",
        "\n",
        "# country_offensive_tweet_map.loc[5] = \"belgium\", \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(belgium)]['tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(belgium)]['offensive_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(belgium)]['ethnicity_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(belgium)]['nationality_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(belgium)]['gender_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(belgium)]['sexual_orientation_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(belgium)]['class_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(belgium)]['religion_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(belgium)]['disability_tweets_received'].sum(), \\\n",
        "#                                      player_offensive_tweet_map[player_offensive_tweet_map['username'].isin(england)]['emoji_tweets_received'].sum(), \\\n",
        "#                                      0\n",
        "                                     \n",
        "# country_offensive_tweet_map.loc[5]['percentage_offensive'] = 100 * (country_offensive_tweet_map.loc[5]['offensive_tweets_received'] / country_offensive_tweet_map.loc[5]['tweets_received'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlTHrCsjKH17"
      },
      "source": [
        "# Create dataframe for analysing abuse by type\n",
        "slurs_by_type = pd.DataFrame(columns=[\"type\", \"total\"])\n",
        "slurs_by_type.loc[0] = \"ethnicity\", joined_df['contains_ethnicity_slurs'].sum()\n",
        "slurs_by_type.loc[1] = \"nationality\", joined_df['contains_nationality_slurs'].sum()\n",
        "slurs_by_type.loc[2] = \"gender\", joined_df['contains_gender_slurs'].sum()\n",
        "slurs_by_type.loc[3] = \"sexual_orientation\", joined_df['contains_sexual_orientation_slurs'].sum()\n",
        "slurs_by_type.loc[4] = \"class\", joined_df['contains_class_slurs'].sum()\n",
        "slurs_by_type.loc[5] = \"religion\", joined_df['contains_religion_slurs'].sum()\n",
        "slurs_by_type.loc[6] = \"disability\", joined_df['contains_disability_slurs'].sum()\n",
        "slurs_by_type.loc[7] = \"emoji\", joined_df['contains_emoji_slurs'].sum()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yECoeCBwvqo1"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aZaw0zgv3Fl",
        "outputId": "aca88cfd-491e-49f4-fd18-1f14f69d4000",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print some headline figures\n",
        "print(f\"Earliest tweet: {joined_df['created_at'].min()}\")\n",
        "print(f\"Latest tweet: {joined_df['created_at'].max()}\")\n",
        "\n",
        "print(f\"Number of tweets (player-only): {len(joined_df)}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Earliest tweet: 2021-06-19 08:10:19+00:00\n",
            "Latest tweet: 2021-07-16 23:59:28+00:00\n",
            "Number of tweets (player-only): 1046319\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRYzUAdJxWtx"
      },
      "source": [
        "# Print emoji stats\n",
        "print(f\"{banana_tweet_count} tweets containing banana emoji. {banana_total} banana emojis used in total\")\n",
        "print(f\"{monkey_tweet_count} tweets containing monkey emoji. {monkey_total} monkey emojis used in total\")\n",
        "print(f\"{monkey_face_tweet_count} tweets containing monkey face emoji. {monkey_face_total} monkey face emojis used in total\")\n",
        "print(f\"{speak_no_evil_monkey_tweet_count} tweets containing speak-no-evil monkey emoji. {speak_no_evil_monkey_total} speak-no-evil monkey emojis used in total\")\n",
        "print(f\"{hear_no_evil_monkey_tweet_count} tweets containing hear-no-evil monkey emoji. {hear_no_evil_monkey_total} hear-no-evil monkey emojis used in total\")\n",
        "print(f\"{see_no_evil_monkey_tweet_count} tweets containing see-no-evil monkey emoji. {see_no_evil_monkey_total} see-no-evil monkey emojis used in total\")\n",
        "print(f\"{gorilla_tweet_count} tweets containing gorilla emoji. {gorilla_total} gorilla emojis used in total\")\n",
        "print(f\"{watermelon_tweet_count} tweets containing watermelon emoji. {watermelon_total} watermelon emojis used in total\")\n",
        "print(f\"{total_emoji_tweets} tweets containing potentially racist emojis. {total_emoji_count} potentially racist emojis used in total\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxC6LuCT2en5",
        "outputId": "c7e5af73-7fd9-4311-daad-1a74414053c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "player_offensive_tweet_map.sort_values('offensive_tweets_received', axis=0, ascending=False, inplace=True)\n",
        "# offensive_tweets_all = player_offensive_tweet_map['offensive_tweets_received'].sum()\n",
        "offensive_tweets_all = joined_df['contains_slurs_or_offensive_emoji'].sum()\n",
        "offensive_tweets_top_10 = player_offensive_tweet_map['offensive_tweets_received'].head(10).sum()\n",
        "top_10_proportion = offensive_tweets_top_10 / offensive_tweets_all\n",
        "print(f\"Top 10 proportion: {top_10_proportion * 100}%\")\n",
        "\n",
        "print(player_offensive_tweet_map[['username', 'offensive_tweets_received']].head(10))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 proportion: 92.8169014084507%\n",
            "           username offensive_tweets_received\n",
            "9    MarcusRashford                      1177\n",
            "22     BukayoSaka87                       845\n",
            "7             HKane                       498\n",
            "14       Sanchooo10                       481\n",
            "8         sterling7                       419\n",
            "5      JackGrealish                       407\n",
            "103   RomeluLukaku9                       234\n",
            "13     OfficialTM_3                       224\n",
            "4    HarryMaguire93                       173\n",
            "81    AntoGriezmann                       155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ypjRKdqqUyS"
      },
      "source": [
        "# Visualisation - wordclouds of slurs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfLU5MiDsK-k"
      },
      "source": [
        "# Menthod for creating a wordcloud, and print out the most frequently used slurs\n",
        "def create_slur_wordcloud(col_name):\n",
        "    list_of_terms = [a for b in joined_df[col_name][joined_df[col_name] != \"[]\"].str.replace(\"'\", \"\").str.replace(\"[\", \"\").str.replace(\"]\", \"\").str.split(\", \") for a in b]\n",
        "    wc = WordCloud(background_color=\"white\", collocations=False).generate(\" \".join(list_of_terms))\n",
        "    plt.figure()\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "    slur_df = pd.DataFrame(columns=[\"term\", \"count\"])\n",
        "    slur_df['term'] = list_of_terms\n",
        "    slur_df['count'] = slur_df['count'].fillna(0)\n",
        "    slur_df = slur_df.groupby(['term']).count()\n",
        "    slur_df = slur_df.sort_values('count', axis=0, ascending=False)\n",
        "    print(slur_df.head(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0HQ5TwRsope"
      },
      "source": [
        "create_slur_wordcloud(\"matching_hatebase_terms_over_50\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puI1roXUs3OB"
      },
      "source": [
        "create_slur_wordcloud(\"matching_hatebase_terms_ethnicity_over_50\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld5hTXI-tlfu"
      },
      "source": [
        "create_slur_wordcloud(\"matching_hatebase_terms_religion_over_50\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NoSEYPmtou3"
      },
      "source": [
        "create_slur_wordcloud(\"matching_hatebase_terms_gender_over_50\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qCnIQ-ltrcJ"
      },
      "source": [
        "create_slur_wordcloud(\"matching_hatebase_terms_sexual_orientation_over_50\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjIygzV-tt76"
      },
      "source": [
        "create_slur_wordcloud(\"matching_hatebase_terms_class_over_50\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMD39uEUtwUz"
      },
      "source": [
        "create_slur_wordcloud(\"matching_hatebase_terms_disability_over_50\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kklMPpK5t59Q"
      },
      "source": [
        "create_slur_wordcloud(\"matching_hatebase_terms_nationality_over_50\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngzVB2h4x6J9"
      },
      "source": [
        "# Visulisations - tweet frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV73ndjoyEAJ"
      },
      "source": [
        "def create_frequency_plot(df, y_values, title, nticks):\n",
        "    fig = go.Figure()\n",
        "    for y_value in y_values:\n",
        "        fig.add_trace(go.Scatter(x=df['created_at'], y=df[y_value],\n",
        "                      mode='lines',\n",
        "                      name=y_value))\n",
        "        \n",
        "    fig.update_layout(\n",
        "        xaxis_title=\"Date\",\n",
        "        yaxis_title=\"Number of tweets\",\n",
        "        xaxis = {\n",
        "          'tickformat': '%d %B',\n",
        "          'tickmode': 'auto',\n",
        "          'nticks': nticks,\n",
        "        },\n",
        "        xaxis_tickformat = '%d %B',\n",
        "        title= {\n",
        "          'text': title,\n",
        "          'y':0.9,\n",
        "          'x':0.5,\n",
        "          'xanchor': 'center',\n",
        "          'yanchor': 'top'\n",
        "        }\n",
        "    )\n",
        "    return fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YKlOpvpzdmJ"
      },
      "source": [
        "INTERVAL = pd.offsets.Minute(60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpvCu7GsziHj"
      },
      "source": [
        "# Plot all tweets frequency\n",
        "df = joined_df.resample(INTERVAL, on='created_at')['tweet_id'].count().reset_index()\n",
        "fig = create_frequency_plot(df, ['tweet_id'], \"Frequency of tweets (aggregated over 60 minute intervals)\", 28)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8558guPQ0QF-"
      },
      "source": [
        "# Plot Sterling, Rashford, Kane tweet frequency\n",
        "df = joined_df.resample(INTERVAL, on='created_at')['MarcusRashford', 'sterling7', 'HKane'].sum().reset_index()\n",
        "fig = create_frequency_plot(df, ['MarcusRashford', 'sterling7', 'HKane'], \"Frequency of tweets (aggregated over 60 minute intervals)\", 28)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B46AVC-r0gD7"
      },
      "source": [
        "# Plot German players\n",
        "df = joined_df.resample(INTERVAL, on='created_at')[germany].sum().reset_index()\n",
        "fig = create_frequency_plot(df, germany, \"Frequency of tweets for German players (aggregated over 60 minute intervals)\", 28)\n",
        "fig.show()\n",
        "\n",
        "# Kroos announced retirement on 2nd July, after they'd been knocked out by England"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GmAA92ROqWk"
      },
      "source": [
        "# Plot Netherlands players\n",
        "df = joined_df.resample(INTERVAL, on='created_at')[netherlands].sum().reset_index()\n",
        "fig = create_frequency_plot(df, netherlands, \"Frequency of tweets for Dutch players (aggregated over 60 minute intervals)\", 28)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtSQLONMOuJ8"
      },
      "source": [
        "# Plot England players\n",
        "df = joined_df.resample(INTERVAL, on='created_at')[england].sum().reset_index()\n",
        "fig = create_frequency_plot(df, england, \"Frequency of tweets for England players (aggregated over 60 minute intervals)\", 28)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpOFutJXOwKl"
      },
      "source": [
        "# Plot Scotland players\n",
        "df = joined_df.resample(INTERVAL, on='created_at')[scotland].sum().reset_index()\n",
        "fig = create_frequency_plot(df, scotland, \"Frequency of tweets for Scottish players (aggregated over 60 minute intervals)\", 28)\n",
        "fig.show()\n",
        "\n",
        "# 25th June - Tierney signs new contract\n",
        "# In days following 18th June - Gilmour has Covid\n",
        "# 2nd July - Gilmour signs for Norwich"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DisTixrMO0Cl"
      },
      "source": [
        "# Plot Belgian players\n",
        "df = joined_df.resample(INTERVAL, on='created_at')[belgium].sum().reset_index()\n",
        "fig = create_frequency_plot(df, belgium, \"Frequency of tweets for Belgian players (aggregated over 60 minute intervals)\", 28)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp8dIjh_O3P4"
      },
      "source": [
        "# Plot French players\n",
        "df = joined_df.resample(INTERVAL, on='created_at')[france].sum().reset_index()\n",
        "fig = create_frequency_plot(df, france, \"Frequency of tweets for French players (aggregated over 60 minute intervals)\", 28)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4kRWC8z04cR"
      },
      "source": [
        "# Plot frequency of offensive tweets\n",
        "df = joined_df.resample(INTERVAL, on='created_at')['contains_slurs_or_offensive_emoji'].sum().reset_index()\n",
        "fig = create_frequency_plot(df, ['contains_slurs_or_offensive_emoji'], \"Frequency of tweets containing HateBase slurs or emoji slurs (aggregated over 60 minute intervals)\", 28)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2dEzfJuXPBX"
      },
      "source": [
        "# Plot frequency of offensive tweets for England players\n",
        "df = joined_df.resample(INTERVAL, on='created_at')[[x + '_offensive' for x in england]].sum().reset_index()\n",
        "fig = create_frequency_plot(df, [x + '_offensive' for x in england], \"Frequency of tweets containing HateBase slurs or emoji slurs for England players (aggregated over 60 minute intervals)\", 28)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMD1EYZqX-sv"
      },
      "source": [
        "# Plot frequency of offensive tweets for Scotland players\n",
        "df = joined_df.resample(INTERVAL, on='created_at')[[x + '_offensive' for x in scotland]].sum().reset_index()\n",
        "fig = create_frequency_plot(df, [x + '_offensive' for x in scotland], \"Frequency of tweets containing HateBase slurs or emoji slurs for Scotland players (aggregated over 60 minute intervals)\", 28)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S23ByRvDYHDt"
      },
      "source": [
        "# Plot frequency of offensive tweets for Belgium players\n",
        "df = joined_df.resample(INTERVAL, on='created_at')[[x + '_offensive' for x in belgium]].sum().reset_index()\n",
        "fig = create_frequency_plot(df, [x + '_offensive' for x in belgium], \"Frequency of tweets containing HateBase slurs or emoji slurs for Belgium players (aggregated over 60 minute intervals)\", 28)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C8HmndkYTFY"
      },
      "source": [
        "# Plot frequency of offensive tweets for France players\n",
        "df = joined_df.resample(INTERVAL, on='created_at')[[x + '_offensive' for x in france]].sum().reset_index()\n",
        "fig = create_frequency_plot(df, [x + '_offensive' for x in france], \"Frequency of tweets containing HateBase slurs or emoji slurs for France players (aggregated over 60 minute intervals)\", 28)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x79SgooBYfDQ"
      },
      "source": [
        "# Plot frequency of offensive tweets for Netherlands players\n",
        "df = joined_df.resample(INTERVAL, on='created_at')[[x + '_offensive' for x in netherlands]].sum().reset_index()\n",
        "fig = create_frequency_plot(df, [x + '_offensive' for x in netherlands], \"Frequency of tweets containing HateBase slurs or emoji slurs for Netherlands players (aggregated over 60 minute intervals)\", 28)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwTxWw1HYuPX"
      },
      "source": [
        "# Plot frequency of offensive tweets for Germany players\n",
        "df = joined_df.resample(INTERVAL, on='created_at')[[x + '_offensive' for x in germany]].sum().reset_index()\n",
        "fig = create_frequency_plot(df, [x + '_offensive' for x in germany], \"Frequency of tweets containing HateBase slurs or emoji slurs for Germany players (aggregated over 60 minute intervals)\", 28)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLGzXt3cG1_x"
      },
      "source": [
        "# Visualisations - stack charts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIcmG4AY_S0r"
      },
      "source": [
        "player_offensive_tweet_map.sort_values('offensive_tweets_received', ascending=False, inplace=True)\n",
        "\n",
        "def create_ethnicity_stack_chart(number):\n",
        "    df = player_offensive_tweet_map.head(number)\n",
        "    data0 = go.Bar(\n",
        "        x = df.username,\n",
        "        y = df.ethnicity_tweets_received,\n",
        "        name = 'Ethnicity-related',\n",
        "        text = df.ethnicity_tweets_received,\n",
        "        textposition = 'inside',\n",
        "        texttemplate = '%{text:.2}'\n",
        "    )\n",
        "\n",
        "    data1 = go.Bar(\n",
        "        x = df.username,\n",
        "        y = df.offensive_tweets_received - df.ethnicity_tweets_received,\n",
        "        name = 'Other',\n",
        "        text = df.offensive_tweets_received - df.ethnicity_tweets_received,\n",
        "        textposition = 'inside',\n",
        "        texttemplate = '%{text:.2}'\n",
        "    )\n",
        "\n",
        "    data = [data0, data1]\n",
        "\n",
        "    layout = go.Layout(title = \"Proportion of tweets containing abusive slurs where slurs related to ethnicity\", barmode='stack')\n",
        "\n",
        "    figure = go.Figure(data = data, layout = layout)\n",
        "    figure.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWA9t2S7D4by"
      },
      "source": [
        "create_ethnicity_stack_chart(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHC43T17HWsh"
      },
      "source": [
        "slurs_by_type.sort_values('total', ascending=False, inplace=True)\n",
        "\n",
        "data = go.Bar(\n",
        "    x = slurs_by_type['type'],\n",
        "    y = slurs_by_type.total,\n",
        "    text = slurs_by_type.total,\n",
        "    textposition = 'outside',\n",
        "    texttemplate = '%{text:.2}'\n",
        ")\n",
        "\n",
        "layout = go.Layout(\n",
        "    title= {\n",
        "          'text': \"Tweets containing slurs by type\",\n",
        "          'y':0.9,\n",
        "          'x':0.5,\n",
        "          'xanchor': 'center',\n",
        "          'yanchor': 'top'\n",
        "        },\n",
        "    yaxis_title=\"Number of tweets\",\n",
        "    )\n",
        "\n",
        "figure = go.Figure(data = data, layout = layout)\n",
        "figure.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
